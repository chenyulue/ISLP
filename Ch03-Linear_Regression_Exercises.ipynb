{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values.\n",
    "\n",
    "    **Solution**: In Table 3.4, the null hypothesis of $H_0$ is that there is no relationship between *sales* and (*TV*, *radio* or *newspaper*). As for the p-value, it refers to the probability of the null hypothesis being true. A small p-value causes us to reject the null hypothesis, which means an association between the predictor and the response.\n",
    "    \n",
    "    As for the p-values in Table 3.4, the p-values for *TV* and *radio* are both less than 0.0001, which means we have to reject the null hypothesis and there is an relationship between *sales* and *TV*, and between *sales* and *radio*. However, the p-value for *newspaper* is 0.8599. It is large enough to make us accept the null hypothesis, and we can conclude that there is no relationship between *sales* and *newspaper*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Carefully explain the diferences between the KNN classifier and KNN regression methods.\n",
    "\n",
    "    **Solution**: The KNN classifier is used for classification. For a given $X_0$, it selects the $K$ observations which are closed to $X_0$, computes the probability of each kind among the $K$ observations, and then labels the $X_0$ the same kind as the observations with the maximum probability of kind.\n",
    "    \n",
    "    In contrast, the KNN regression is used to predict quantative values. It selects the $K$ observations nearest to the input $X_0$, and computes the average response values of all the $K$ observations, which are the predicted response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suppose we have a data set with five predictors, $X_1 = GPA$, $X_2 = IQ$, $X_3 = Gender$ (1 for Female and 0 for Male), $X_4\\;=Interaction\\;between\\;GPA\\;and \\;Q$, and $X_5\\;=\\;Interaction\\;between\\;GPA\\;and\\;Gender$. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\\hat{\\beta}_0=50,\\;\\hat{\\beta}_1=20,\\;\\hat{\\beta}_2=0.07,\\;\\hat{\\beta}_3=35,\\;\\hat{\\beta}_4=0.01,\\;\\hat{\\beta}_5=-10$.\n",
    "\n",
    "    (a) Which answer is correct, and why?\n",
    "    \n",
    "    i. For a fixed value of IQ and GPA, males earn more on average than females.\n",
    "    \n",
    "    ii. For a fixed value of IQ and GPA, females earn more on average than males.\n",
    "    \n",
    "    iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.\n",
    "    \n",
    "    iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.\n",
    "    \n",
    "    **Solution**: \n",
    "    $Salary_{male} = 50+20GPA+0.07IQ+0.01GPA\\times IQ$\n",
    "    \n",
    "    $Salary_{female} = 50+20GPA+0.07IQ+35+0.01GPA\\times IQ-10GPA = 85 + 10GPA + 0.07IQ + 0.01GPA\\times IQ$\n",
    "    \n",
    "    $\\Delta Slary = Salary_{female} - Salary_{male} = 35 - 10GPA$\n",
    "    \n",
    "    From the formula shown above, we can see whether males earn more on average than females depends on the GPA value. For a fixed value of IQ and GPA, if GPA is high enough to make $\\Delta Salary$ less than 0, then males earn more; otherwise, females earn more.\n",
    "    \n",
    "    Therefore, only the statement *iii* is correct.\n",
    "    \n",
    "    (b) Predict the salary of a female wit IQ of 110 and a GPA of 4.0.\n",
    "    \n",
    "    **Solution**: According to the formular shown as above, $Salary_{female} = 85 + 10\\times 4.0 + 0.07\\times 110 + 0.01\\times 110 \\times 4.0 = 137.1$\n",
    "    \n",
    "    (c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n",
    "    \n",
    "    **Solution**: It's false, because whether there is an interaction effect of GPA/IQ or not does not depend on the magnitude of the corresponding coefficient, but depends on the p-value for the coefficient. If the p-value of the GPA/IQ interaction term is large, then we can conclude that there is little evidence of an interaction effect. But it's wrong to draw such an conclusion according to the magnitude of the cofficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. I collect a set of data ($n=100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a reparate cubic regression, i.e. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$.\n",
    "    \n",
    "    (a) Suppose that the true relationsihp between $X$ and $Y$ is linear, i.e. $Y = \\beta_0 + \\beta_1 Y + \\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "    \n",
    "    **Solution**: For training data, RSS decreases as the model complexity increases, because the model will try its best to find the pattern and cause overfitting of the data. In this case, the cubic regression will try to follow the data more closely, and hence, RSS for cubic regression will be lower than that for the linear regression.\n",
    "    \n",
    "    (b) Answer (a) using test rather than training RSS.\n",
    "    \n",
    "    **Solution**: Since the true relationship between $X$ and $Y$ is linear, the linear regression can find the pattern close to the true relationship, whereas the cubic regression overfits the data and find a totally different relationship from the true one. Therefore, the linear regression will perform better on the test data, and the RSS for the linear regression will be lower than that for the cubic regression when using test data.\n",
    "    \n",
    "    (c) Suppose that the true relationship between $X$ and $Y$ is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "    \n",
    "    **Solution**: The RSS for the cubic regression will be still lower than that for the linear regression due to the same reason as (a).\n",
    "    \n",
    "    (d) Answer (c) using test rather than training RSS.\n",
    "    \n",
    "    **Solution**: There is not enough information to tell, because we don't know the true relationship, and thus we can't tell which one is more close to the true pattern. The RSS for the model which is more close to the true relationship will be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the *i*th fitted value takes te form \n",
    "\n",
    "    $\\hat{y}_i = x_i \\hat{\\beta}$\n",
    "    \n",
    "    where\n",
    "    \n",
    "    $\\hat{\\beta} = (\\sum_{i=1}^n x_i y_i)/(\\sum_{i'=1}^n x_{i'}^2)$\n",
    "    \n",
    "    Show that we can write\n",
    "    \n",
    "    $\\hat{y}_i = \\sum_{i'=1}^n a_{i'} y_{i'}$\n",
    "    \n",
    "    What is $a_{i'}$?\n",
    "    \n",
    "    **Solution**: \n",
    "    \n",
    "    $\\hat{y}_i = x_i \\hat{\\beta} = (\\sum_{j=1}^n x_i x_j y_j)/(\\sum_{j'=1}^n x_{j'}^2) = \\sum_{j=1}^n \\frac{x_i x_j y_j}{\\sum_{j'=1}^n x_{j'}^2} = \\sum_{j=1}^n a_j y_j$\n",
    "    \n",
    "    where\n",
    "    \n",
    "    $a_j = \\frac{x_i x_j}{\\sum_{j'=1}^n x_{j'}^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\\bar{x}, \\bar{y})$.\n",
    "\n",
    "    **Solution**:\n",
    "    \n",
    "    $\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$\n",
    "    \n",
    "    $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$\n",
    "    \n",
    "    $\\hat{f}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $\\hat{f}(\\bar{x}) = \\hat{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 \\bar{x}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $\\hat{f}(\\bar{x}) = \\hat{y}$\n",
    "    \n",
    "    which means that the least squares line always passes through the point $(\\bar{x}, \\bar{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. It is claimed in the text that in the case of simple linear regression of $Y$ onto $X$, the $R^2$ statistic (3.7) is equal to the square of the correlation between $X$ and $Y$ (3.18). Prove that this is the case. For simplicity, you may assume that $\\bar{x}=\\bar{y}=0$.\n",
    "\n",
    "    **Solution**:\n",
    "    \n",
    "    $R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}$\n",
    "    \n",
    "    $TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2$\n",
    "    \n",
    "    $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2$\n",
    "    \n",
    "    $\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$\n",
    "    \n",
    "    $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$\n",
    "    \n",
    "    => \n",
    "    \n",
    "    $R^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2 - \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2 - \\sum_{i=1}^n (y_i - \\bar{y} + \\hat{\\beta}_1 \\bar{x} - \\hat{\\beta}_1 x_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2 - \\sum_{i=1}^n (y_i - \\bar{y} - \\hat{\\beta}_1 (x_i - \\bar{x}))^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    => \n",
    "    \n",
    "    $R^2 = \\frac{\\sum_{i=1}^n [(y_i - \\bar{y})^2 - (y_i - \\bar{y} - \\hat{\\beta}_1 (x_i - \\bar{x}))^2]}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\sum_{i=1}^n 2\\hat{\\beta}_1(x_i - \\bar{x}) (y_i - \\bar{y}) - \\hat{\\beta}_1^2 (x_i - \\bar{x})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    => \n",
    "    \n",
    "    $R^2 = \\frac{\\hat{\\beta}_1 \\sum_{i=1}^n 2(x_i - \\bar{x}) (y_i - \\bar{y}) - \\hat{\\beta}_1 (x_i - \\bar{x})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\hat{\\beta}_1 [\\sum_{i=1}^n 2(x_i - \\bar{x}) (y_i - \\bar{y}) - \\sum_{i=1}^n \\hat{\\beta}_1 (x_i - \\bar{x})^2]}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\hat{\\beta}_1 [\\sum_{i=1}^n 2(x_i - \\bar{x}) (y_i - \\bar{y}) - \\sum_{i=1}^n \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} (x_i - \\bar{x})^2]}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\hat{\\beta}_1 [\\sum_{i=1}^n 2(x_i - \\bar{x}) (y_i - \\bar{y}) - \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sum_{i=1}^n (x_i - \\bar{x})^2]}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\hat{\\beta}_1 [\\sum_{i=1}^n 2(x_i - \\bar{x}) (y_i - \\bar{y}) - \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})]}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\hat{\\beta}_1 \\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = \\frac{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$\n",
    "    \n",
    "    =>\n",
    "    \n",
    "    $R^2 = {Cor(X, Y)}^2$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. This question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
