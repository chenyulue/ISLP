{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual\n",
    "\n",
    "1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3).\n",
    "\n",
    "**Solution**:\n",
    "$$\\begin{align} p(X) &= \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1}{p(X)} &= \\frac{1 + e^{\\beta_0 + \\beta_1 X}}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1}{p(X)} &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} + 1 \\\\ \\Longrightarrow \\frac{1}{p(X)} - 1 &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1 - p(X)}{p(X)} &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{p(X)}{1 - p(X)} &= e^{\\beta_0 + \\beta_1 X}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prove the state that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} p_k(x) &= \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)} \\\\ \\Longrightarrow log(p_k(x)) &= log(\\pi_k) - \\frac{1}{2\\sigma^2}(x-\\mu_k)^2 - log(C) \\\\ \\Longrightarrow log(p_k(x)) + log(C) &= log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} - \\frac{x^2}{2\\sigma^2} \\\\ \\Longrightarrow log(p_k(x)) + log(C) + \\frac{x^2}{2\\sigma^2}&= log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\end{align}$$\n",
    "\n",
    "where\n",
    "$$ C = \\sum_{l=1}^K \\pi_l exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)$$\n",
    "\n",
    "\n",
    "For a specific observation $x$, $C$ and $\\frac{x^2}{2\\sigma^2}$ are constants, so supposing $\\delta_k(x) = log(p_k(x)) + log(C) + \\frac{x^2}{2\\sigma^2}$, we get:\n",
    "$$\\delta_k(x) = log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}$$\n",
    "\n",
    "It is clearly concluded that $p_k(x)$ is largest $\\iff$ $\\delta_k(x)$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This problem relates to the QDA model with only one feature; that is, $p=1$. Suppose that we have $K$ classes, and that if an observation belongs to the *k*-th class then $X$ comes from a one-dimentional normal distribution, $X\\,\\sim\\,N(\\mu_k, \\sigma_k^2)$. According to (4.11), prove that in this case without making the assumption that $\\sigma_1^2 = ... = \\sigma_K^2$, the Bayes' classifier is *not* linear. Argue that it is in fact quadratic.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} f_k(x) &= \\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2) \\\\ p_k(x) &= \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\\\ \\Longrightarrow p_k(x) &= \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma_l}exp(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2)} \\\\ \\Longrightarrow log(p_k(x)) &= log(\\pi_k) - log(\\sigma_k) - \\frac{1}{2\\sigma_k^2} (x-\\mu_k)^2 - log(C) \\\\ \\Longrightarrow log(p_k(x)) + log(C) &= log(\\pi_k) - log(\\sigma_k) - \\frac{x^2}{2\\sigma_k^2} + x \\cdot \\frac{\\mu_k}{2\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "$$C=\\sum_{l=1}^K \\pi_l \\frac{1}{\\sigma_l}exp(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2)$$\n",
    "\n",
    "For a specific observation, $C$ is a constant, so we let $\\delta_k(x) = log(p_k(x)) + log(C)$, and get the following formula:\n",
    "\n",
    "$$\\delta_k(x) = log(\\pi_k) - log(\\sigma_k) - \\frac{x^2}{2\\sigma_k^2} + x \\cdot \\frac{\\mu_k}{2\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2}$$\n",
    "\n",
    "We can see that the Bayes' classifier is not linear; actually, it is a quadratic function of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We will now investigate the phenomenon known as the *curse of dimensionality*, which ties into the fact that non-parametric approaches often perform poorly when *p* is large.\n",
    "\n",
    "(a) For the case of $p=1$, what fraction of the available observations will we use to make the prediction on average?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since $X$ is uniformly distributed on $[0,1]$, we have to analyze this problem in three different case:\n",
    "\n",
    "* If $X \\in [0.05, 0.95]$, the observations that will be used to make a prediction are in the interval $[x-0.05, x+0.05]$, and clearly $10\\%$ of the available observations will be used.\n",
    "* If $X \\in [0, 0.05]$, then we will use the observations that are in the interval $[0, x+0.05]$; in this case, the fraction of the available observations that we will use is $(100x+5)\\%$.\n",
    "* If $ X \\in [0.95, 1]$, the observations that are in the interval $[x-0.05, 1]$ are used to make the prediction, so the fraction of the available observations used is $(105-100x)\\%$.\n",
    "\n",
    "In total, the average fraction of the available observations that we will use is:\n",
    "\n",
    "$$\\int_{0.05}^{0.95}10dx + \\int_0^{0.05} (100x+5)dx + \\int_{0.95}^1 (105-100x)dx = 9 + 0.375 + 0.375 = 9.75\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For the case of $p=2$, what fraction of the available observations will we use to make the prediction on average?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "If $X_1$ and $X_2$ are independent, the average fraction of the available observations we will use to make the prediction is $9.75\\% \\times 9.75\\% =  95.0625\\%$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) For the case of $p=100$, what fraction of the available observations will we use to make the prediction?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "With the same assumption as (a) and (b), the fraction of the available observations is $9.75\\%^{100} \\approx 0\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Using your answers to parts (a)-(c), argue that a drawback of KNN when *p* is large is that there are very few training observations \"near\" any given test observation.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "As we see from (a)-(c), the fraction of the available observations that we will use to make the prediction is $(9.75\\%)^p$ with $p$ as the number of features. So when $p$ is large, $(9.75\\%)^p$ has a tendency to zero, which means there are very few training observations *near* any given test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) For $p=1,\\,2,$ and $100$, what is the length of each side of the hypercube? Comment on your answer.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since in this case we do not make any assumption about the distribution of $X$, so for $p=1$, we have $l=0.1$; for $p=2$, we have $l=0.1^{1/2}$; and for $p=100$, we have $l=0.1^{1/100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We now examine the differences between LDA and QDA.\n",
    "\n",
    "(a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "For the training set, QDA should perform better than LDA, but worse for the test set, because QDA may produce a closer fit with its high flexibility but could overfit the linearity on the Bayes decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "If the Bayes decision boundary is non-linear, we expect QDA to perform better both on the training set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) In general, as the sample size n increases, do we expect the test perdiction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since QDA has a higher flexibility than LDA and so has a higher variance, QDA generally performs better if the sample size n is very large. In this case, the variance of the classifier is not a major concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) True or False, and justify your answer.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "False. Whether QDA achieves a superior test error rate or not depends on the sample size. For a small sample size, a more flexible method, such as QDA, may overfit the data set, which in turn leads to an inferior test error. For a sample with a very large size, QDA may perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
