{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual\n",
    "\n",
    "1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3).\n",
    "\n",
    "**Solution**:\n",
    "$$\\begin{align} p(X) &= \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1}{p(X)} &= \\frac{1 + e^{\\beta_0 + \\beta_1 X}}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1}{p(X)} &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} + 1 \\\\ \\Longrightarrow \\frac{1}{p(X)} - 1 &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1 - p(X)}{p(X)} &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{p(X)}{1 - p(X)} &= e^{\\beta_0 + \\beta_1 X}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prove the state that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} p_k(x) &= \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)} \\\\ \\Longrightarrow log(p_k(x)) &= log(\\pi_k) - \\frac{1}{2\\sigma^2}(x-\\mu_k)^2 - log(C) \\\\ \\Longrightarrow log(p_k(x)) + log(C) &= log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} - \\frac{x^2}{2\\sigma^2} \\\\ \\Longrightarrow log(p_k(x)) + log(C) + \\frac{x^2}{2\\sigma^2}&= log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\end{align}$$\n",
    "\n",
    "where\n",
    "$$ C = \\sum_{l=1}^K \\pi_l exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)$$\n",
    "\n",
    "\n",
    "For a specific observation $x$, $C$ and $\\frac{x^2}{2\\sigma^2}$ are constants, so supposing $\\delta_k(x) = log(p_k(x)) + log(C) + \\frac{x^2}{2\\sigma^2}$, we get:\n",
    "$$\\delta_k(x) = log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}$$\n",
    "\n",
    "It is clearly concluded that $p_k(x)$ is largest $\\iff$ $\\delta_k(x)$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This problem relates to the QDA model with only one feature; that is, $p=1$. Suppose that we have $K$ classes, and that if an observation belongs to the *k*-th class then $X$ comes from a one-dimentional normal distribution, $X\\,\\sim\\,N(\\mu_k, \\sigma_k^2)$. According to (4.11), prove that in this case without making the assumption that $\\sigma_1^2 = ... = \\sigma_K^2$, the Bayes' classifier is *not* linear. Argue that it is in fact quadratic.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} f_k(x) &= \\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2) \\\\ p_k(x) &= \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\\\ \\Longrightarrow p_k(x) &= \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma_l}exp(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2)} \\\\ \\Longrightarrow log(p_k(x)) &= log(\\pi_k) - log(\\sigma_k) - \\frac{1}{2\\sigma_k^2} (x-\\mu_k)^2 - log(C) \\\\ \\Longrightarrow log(p_k(x)) + log(C) &= log(\\pi_k) - log(\\sigma_k) - \\frac{x^2}{2\\sigma_k^2} + x \\cdot \\frac{\\mu_k}{2\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "$$C=\\sum_{l=1}^K \\pi_l \\frac{1}{\\sigma_l}exp(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2)$$\n",
    "\n",
    "For a specific observation, $C$ is a constant, so we let $\\delta_k(x) = log(p_k(x)) + log(C)$, and get the following formula:\n",
    "\n",
    "$$\\delta_k(x) = log(\\pi_k) - log(\\sigma_k) - \\frac{x^2}{2\\sigma_k^2} + x \\cdot \\frac{\\mu_k}{2\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2}$$\n",
    "\n",
    "We can see that the Bayes' classifier is not linear; actually, it is a quadratic function of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We will now investigate the phenomenon known as the *curse of dimensionality*, which ties into the fact that non-parametric approaches often perform poorly when *p* is large.\n",
    "\n",
    "(a) For the case of $p=1$, what fraction of the available observations will we use to make the prediction on average?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since $X$ is uniformly distributed on $[0,1]$, we have to analyze this problem in three different case:\n",
    "\n",
    "* If $X \\in [0.05, 0.95]$, the observations that will be used to make a prediction are in the interval $[x-0.05, x+0.05]$, and clearly $10\\%$ of the available observations will be used.\n",
    "* If $X \\in [0, 0.05]$, then we will use the observations that are in the interval $[0, x+0.05]$; in this case, the fraction of the available observations that we will use is $(100x+5)\\%$.\n",
    "* If $ X \\in [0.95, 1]$, the observations that are in the interval $[x-0.05, 1]$ are used to make the prediction, so the fraction of the available observations used is $(105-100x)\\%$.\n",
    "\n",
    "In total, the average fraction of the available observations that we will use is:\n",
    "\n",
    "$$\\int_{0.05}^{0.95}10dx + \\int_0^{0.05} (100x+5)dx + \\int_{0.95}^1 (105-100x)dx = 9 + 0.375 + 0.375 = 9.75\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For the case of $p=2$, what fraction of the available observations will we use to make the prediction on average?\n",
    "\n",
    "**Solution**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
