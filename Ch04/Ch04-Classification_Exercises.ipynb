{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual\n",
    "\n",
    "1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3).\n",
    "\n",
    "**Solution**:\n",
    "$$\\begin{align} p(X) &= \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1}{p(X)} &= \\frac{1 + e^{\\beta_0 + \\beta_1 X}}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1}{p(X)} &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} + 1 \\\\ \\Longrightarrow \\frac{1}{p(X)} - 1 &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{1 - p(X)}{p(X)} &= \\frac{1}{e^{\\beta_0 + \\beta_1 X}} \\\\ \\Longrightarrow \\frac{p(X)}{1 - p(X)} &= e^{\\beta_0 + \\beta_1 X}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prove the state that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} p_k(x) &= \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)} \\\\ \\Longrightarrow log(p_k(x)) &= log(\\pi_k) - \\frac{1}{2\\sigma^2}(x-\\mu_k)^2 - log(C) \\\\ \\Longrightarrow log(p_k(x)) + log(C) &= log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} - \\frac{x^2}{2\\sigma^2} \\\\ \\Longrightarrow log(p_k(x)) + log(C) + \\frac{x^2}{2\\sigma^2}&= log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\end{align}$$\n",
    "\n",
    "where\n",
    "$$ C = \\sum_{l=1}^K \\pi_l exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)$$\n",
    "\n",
    "\n",
    "For a specific observation $x$, $C$ and $\\frac{x^2}{2\\sigma^2}$ are constants, so supposing $\\delta_k(x) = log(p_k(x)) + log(C) + \\frac{x^2}{2\\sigma^2}$, we get:\n",
    "$$\\delta_k(x) = log(\\pi_k) + x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2}$$\n",
    "\n",
    "It is clearly concluded that $p_k(x)$ is largest $\\iff$ $\\delta_k(x)$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This problem relates to the QDA model with only one feature; that is, $p=1$. Suppose that we have $K$ classes, and that if an observation belongs to the *k*-th class then $X$ comes from a one-dimentional normal distribution, $X\\,\\sim\\,N(\\mu_k, \\sigma_k^2)$. According to (4.11), prove that in this case without making the assumption that $\\sigma_1^2 = ... = \\sigma_K^2$, the Bayes' classifier is *not* linear. Argue that it is in fact quadratic.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} f_k(x) &= \\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2) \\\\ p_k(x) &= \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} \\\\ \\Longrightarrow p_k(x) &= \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp(-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma_l}exp(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2)} \\\\ \\Longrightarrow log(p_k(x)) &= log(\\pi_k) - log(\\sigma_k) - \\frac{1}{2\\sigma_k^2} (x-\\mu_k)^2 - log(C) \\\\ \\Longrightarrow log(p_k(x)) + log(C) &= log(\\pi_k) - log(\\sigma_k) - \\frac{x^2}{2\\sigma_k^2} + x \\cdot \\frac{\\mu_k}{2\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "$$C=\\sum_{l=1}^K \\pi_l \\frac{1}{\\sigma_l}exp(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2)$$\n",
    "\n",
    "For a specific observation, $C$ is a constant, so we let $\\delta_k(x) = log(p_k(x)) + log(C)$, and get the following formula:\n",
    "\n",
    "$$\\delta_k(x) = log(\\pi_k) - log(\\sigma_k) - \\frac{x^2}{2\\sigma_k^2} + x \\cdot \\frac{\\mu_k}{2\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2}$$\n",
    "\n",
    "We can see that the Bayes' classifier is not linear; actually, it is a quadratic function of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We will now investigate the phenomenon known as the *curse of dimensionality*, which ties into the fact that non-parametric approaches often perform poorly when *p* is large.\n",
    "\n",
    "(a) For the case of $p=1$, what fraction of the available observations will we use to make the prediction on average?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since $X$ is uniformly distributed on $[0,1]$, we have to analyze this problem in three different case:\n",
    "\n",
    "* If $X \\in [0.05, 0.95]$, the observations that will be used to make a prediction are in the interval $[x-0.05, x+0.05]$, and clearly $10\\%$ of the available observations will be used.\n",
    "* If $X \\in [0, 0.05]$, then we will use the observations that are in the interval $[0, x+0.05]$; in this case, the fraction of the available observations that we will use is $(100x+5)\\%$.\n",
    "* If $ X \\in [0.95, 1]$, the observations that are in the interval $[x-0.05, 1]$ are used to make the prediction, so the fraction of the available observations used is $(105-100x)\\%$.\n",
    "\n",
    "In total, the average fraction of the available observations that we will use is:\n",
    "\n",
    "$$\\int_{0.05}^{0.95}10dx + \\int_0^{0.05} (100x+5)dx + \\int_{0.95}^1 (105-100x)dx = 9 + 0.375 + 0.375 = 9.75\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For the case of $p=2$, what fraction of the available observations will we use to make the prediction on average?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "If $X_1$ and $X_2$ are independent, the average fraction of the available observations we will use to make the prediction is $9.75\\% \\times 9.75\\% =  95.0625\\%$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) For the case of $p=100$, what fraction of the available observations will we use to make the prediction?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "With the same assumption as (a) and (b), the fraction of the available observations is $9.75\\%^{100} \\approx 0\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Using your answers to parts (a)-(c), argue that a drawback of KNN when *p* is large is that there are very few training observations \"near\" any given test observation.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "As we see from (a)-(c), the fraction of the available observations that we will use to make the prediction is $(9.75\\%)^p$ with $p$ as the number of features. So when $p$ is large, $(9.75\\%)^p$ has a tendency to zero, which means there are very few training observations *near* any given test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) For $p=1,\\,2,$ and $100$, what is the length of each side of the hypercube? Comment on your answer.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since in this case we do not make any assumption about the distribution of $X$, so for $p=1$, we have $l=0.1$; for $p=2$, we have $l=0.1^{1/2}$; and for $p=100$, we have $l=0.1^{1/100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We now examine the differences between LDA and QDA.\n",
    "\n",
    "(a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "For the training set, QDA should perform better than LDA, but worse for the test set, because QDA may produce a closer fit with its high flexibility but could overfit the linearity on the Bayes decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "If the Bayes decision boundary is non-linear, we expect QDA to perform better both on the training set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) In general, as the sample size n increases, do we expect the test perdiction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since QDA has a higher flexibility than LDA and so has a higher variance, QDA generally performs better if the sample size n is very large. In this case, the variance of the classifier is not a major concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) True or False, and justify your answer.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "False. Whether QDA achieves a superior test error rate or not depends on the sample size. For a small sample size, a more flexible method, such as QDA, may overfit the data set, which in turn leads to an inferior test error. For a sample with a very large size, QDA may perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Suppose we collect data for a group of students in a statistics class with variables $X_1=hours\\; studied$, $X_2 = undergrad \\; GPA$, and $Y = receive \\; an\\; A$.\n",
    "\n",
    "(a) Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} log(\\frac{p_A(x)}{1-p_A(x)}) &= -6 + 0.05 X_1 + X_2 \\\\ &= -6 + 0.05 \\times 40 + 3.5 \\\\ &= -0.5 \\\\ \\Longrightarrow \\frac{p_A(x)}{1-p_A(x)} = e^{-0.5} \\\\ \\Longrightarrow p_A(x) = 0.378 \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$\\begin{align} log(1) &= -6 + 0.05X_1 + 3.5 \\\\ \\Longrightarrow 0 &= 0.05X_1 - 2.5 \\\\ \\Longrightarrow X_1 &= 50 \\end{align}$$\n",
    "\n",
    "So 50 hours are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Suppose that we wish to predict whether a given stock will issue a dividend this year (\"Yes\" or \"No\") based on $X$, last year's percent profit.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "According to the description, we can obtain the following parameters:\n",
    "\n",
    "* $\\pi_{Yes} = 0.8,\\; \\pi_{No} = 0.2$\n",
    "* $\\mu_{Yes} = 10,\\; \\mu_{No} = 0,\\; \\hat{delta}^2=36$\n",
    "\n",
    "Pass the above parameters into the Bayes' theorm formula:\n",
    "\n",
    "$$Pr(Y=Yes|X=4) = \\frac{0.8\\times exp(-1/2)}{0.8 \\times exp(-1/2) + 0.2 \\times exp(-2/9)} = 0.7519$$\n",
    "\n",
    "So the probability that a company will issue a dividend this year is 75.19% given that its percentage profit was $X=4$ last year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Based on these resuls, which method should we prefer to use for classification of new observations? Why?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "In the case of KNN method with $K=1$, the probability is given as:\n",
    "\n",
    "$$Pr(Y=j|X=x_0) = I(y_i=j)$$\n",
    "\n",
    "which is equal to 1 if $y_i=j$ and 0 if not. Therefore we do not make any error on the training data within this setting, which means we have a training error rate of 0%. \n",
    "\n",
    "However we have an average error rate of 18% for the KNN method, which means that the test error rate is 36%. This test error rate is greater than that for the logistic regression. \n",
    "\n",
    "So it's better to choose the logistic regression for classification of new observations due to its lower test error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. This problem has to do with *odds*.\n",
    "\n",
    "(a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "For *odds*, we write it as:\n",
    "\n",
    "$$\\frac{p(X)}{1-p(X)} = 0.37$$\n",
    "\n",
    "Compute it, and we get $p(X) = 0.37/(1+0.37) = 0.27$\n",
    "\n",
    "(b) Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "$$odds = \\frac{p(X)}{1-p(X)} = \\frac{0.16}{1-0.16} = 0.19$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to import some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn import neighbors, preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. This question should be answered using the **Weekly** data set.\n",
    "\n",
    "(a) Produce some numerial and graphical summaries of the **Weekly** data. Do there appear to be any patterns?\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>0.159837</td>\n",
       "      <td>3.514</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.712</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.712</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.153728</td>\n",
       "      <td>1.178</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction\n",
       "0  1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down\n",
       "1  1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down\n",
       "2  1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514        Up\n",
       "3  1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712        Up\n",
       "4  1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178        Up"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly = pd.read_csv('../data/Weekly.csv',\n",
    "                    na_values='?')\n",
    "weekly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2000.048669</td>\n",
       "      <td>0.150585</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.147205</td>\n",
       "      <td>0.145818</td>\n",
       "      <td>0.139893</td>\n",
       "      <td>1.574618</td>\n",
       "      <td>0.149899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.033182</td>\n",
       "      <td>2.357013</td>\n",
       "      <td>2.357254</td>\n",
       "      <td>2.360502</td>\n",
       "      <td>2.360279</td>\n",
       "      <td>2.361285</td>\n",
       "      <td>1.686636</td>\n",
       "      <td>2.356927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1990.000000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>0.087465</td>\n",
       "      <td>-18.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1995.000000</td>\n",
       "      <td>-1.154000</td>\n",
       "      <td>-1.154000</td>\n",
       "      <td>-1.158000</td>\n",
       "      <td>-1.158000</td>\n",
       "      <td>-1.166000</td>\n",
       "      <td>0.332022</td>\n",
       "      <td>-1.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>1.002680</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>1.405000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.405000</td>\n",
       "      <td>2.053727</td>\n",
       "      <td>1.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>9.328214</td>\n",
       "      <td>12.026000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Year         Lag1         Lag2         Lag3         Lag4  \\\n",
       "count  1089.000000  1089.000000  1089.000000  1089.000000  1089.000000   \n",
       "mean   2000.048669     0.150585     0.151079     0.147205     0.145818   \n",
       "std       6.033182     2.357013     2.357254     2.360502     2.360279   \n",
       "min    1990.000000   -18.195000   -18.195000   -18.195000   -18.195000   \n",
       "25%    1995.000000    -1.154000    -1.154000    -1.158000    -1.158000   \n",
       "50%    2000.000000     0.241000     0.241000     0.241000     0.238000   \n",
       "75%    2005.000000     1.405000     1.409000     1.409000     1.409000   \n",
       "max    2010.000000    12.026000    12.026000    12.026000    12.026000   \n",
       "\n",
       "              Lag5       Volume        Today  \n",
       "count  1089.000000  1089.000000  1089.000000  \n",
       "mean      0.139893     1.574618     0.149899  \n",
       "std       2.361285     1.686636     2.356927  \n",
       "min     -18.195000     0.087465   -18.195000  \n",
       "25%      -1.166000     0.332022    -1.154000  \n",
       "50%       0.234000     1.002680     0.241000  \n",
       "75%       1.405000     2.053727     1.405000  \n",
       "max      12.026000     9.328214    12.026000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.032289</td>\n",
       "      <td>-0.033390</td>\n",
       "      <td>-0.030006</td>\n",
       "      <td>-0.031128</td>\n",
       "      <td>-0.030519</td>\n",
       "      <td>0.841942</td>\n",
       "      <td>-0.032460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>-0.032289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074853</td>\n",
       "      <td>0.058636</td>\n",
       "      <td>-0.071274</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.064951</td>\n",
       "      <td>-0.075032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>-0.033390</td>\n",
       "      <td>-0.074853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.075721</td>\n",
       "      <td>0.058382</td>\n",
       "      <td>-0.072499</td>\n",
       "      <td>-0.085513</td>\n",
       "      <td>0.059167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>-0.030006</td>\n",
       "      <td>0.058636</td>\n",
       "      <td>-0.075721</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.075396</td>\n",
       "      <td>0.060657</td>\n",
       "      <td>-0.069288</td>\n",
       "      <td>-0.071244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>-0.031128</td>\n",
       "      <td>-0.071274</td>\n",
       "      <td>0.058382</td>\n",
       "      <td>-0.075396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.075675</td>\n",
       "      <td>-0.061075</td>\n",
       "      <td>-0.007826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>-0.030519</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.072499</td>\n",
       "      <td>0.060657</td>\n",
       "      <td>-0.075675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.058517</td>\n",
       "      <td>0.011013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.841942</td>\n",
       "      <td>-0.064951</td>\n",
       "      <td>-0.085513</td>\n",
       "      <td>-0.069288</td>\n",
       "      <td>-0.061075</td>\n",
       "      <td>-0.058517</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.033078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>-0.032460</td>\n",
       "      <td>-0.075032</td>\n",
       "      <td>0.059167</td>\n",
       "      <td>-0.071244</td>\n",
       "      <td>-0.007826</td>\n",
       "      <td>0.011013</td>\n",
       "      <td>-0.033078</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000 -0.032289 -0.033390 -0.030006 -0.031128 -0.030519  0.841942   \n",
       "Lag1   -0.032289  1.000000 -0.074853  0.058636 -0.071274 -0.008183 -0.064951   \n",
       "Lag2   -0.033390 -0.074853  1.000000 -0.075721  0.058382 -0.072499 -0.085513   \n",
       "Lag3   -0.030006  0.058636 -0.075721  1.000000 -0.075396  0.060657 -0.069288   \n",
       "Lag4   -0.031128 -0.071274  0.058382 -0.075396  1.000000 -0.075675 -0.061075   \n",
       "Lag5   -0.030519 -0.008183 -0.072499  0.060657 -0.075675  1.000000 -0.058517   \n",
       "Volume  0.841942 -0.064951 -0.085513 -0.069288 -0.061075 -0.058517  1.000000   \n",
       "Today  -0.032460 -0.075032  0.059167 -0.071244 -0.007826  0.011013 -0.033078   \n",
       "\n",
       "           Today  \n",
       "Year   -0.032460  \n",
       "Lag1   -0.075032  \n",
       "Lag2    0.059167  \n",
       "Lag3   -0.071244  \n",
       "Lag4   -0.007826  \n",
       "Lag5    0.011013  \n",
       "Volume -0.033078  \n",
       "Today   1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly.iloc[:, :-1].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=weekly.index, y=weekly.Volume);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations between the lag variables and todays returns are close to zero. The only substantial correlation is between Year and Volume. When we plot Volume vs index, we see that it is increasing over time.\n",
    "\n",
    "(b) Use the full data set to perform a logistic regression with **Direction** as the response and the five lag variables plus **Volume** as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>['Direction[Down]', 'Direction[Up]']</td> <th>  No. Observations:  </th>  <td>  1089</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                            <td>GLM</td>                 <th>  Df Residuals:      </th>  <td>  1082</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>                  <td>Binomial</td>               <th>  Df Model:          </th>  <td>     6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>                   <td>logit</td>                <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                          <td>IRLS</td>                 <th>  Log-Likelihood:    </th> <td> -743.18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                      <td>Sat, 02 Jan 2021</td>           <th>  Deviance:          </th> <td>  1486.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                          <td>21:41:47</td>               <th>  Pearson chi2:      </th> <td>1.09e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>                    <td>4</td>                  <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>               <td>nonrobust</td>              <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.2669</td> <td>    0.086</td> <td>   -3.106</td> <td> 0.002</td> <td>   -0.435</td> <td>   -0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>    0.0413</td> <td>    0.026</td> <td>    1.563</td> <td> 0.118</td> <td>   -0.010</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0584</td> <td>    0.027</td> <td>   -2.175</td> <td> 0.030</td> <td>   -0.111</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag3</th>      <td>    0.0161</td> <td>    0.027</td> <td>    0.602</td> <td> 0.547</td> <td>   -0.036</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag4</th>      <td>    0.0278</td> <td>    0.026</td> <td>    1.050</td> <td> 0.294</td> <td>   -0.024</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag5</th>      <td>    0.0145</td> <td>    0.026</td> <td>    0.549</td> <td> 0.583</td> <td>   -0.037</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Volume</th>    <td>    0.0227</td> <td>    0.037</td> <td>    0.616</td> <td> 0.538</td> <td>   -0.050</td> <td>    0.095</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Generalized Linear Model Regression Results                           \n",
       "================================================================================================\n",
       "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                 1089\n",
       "Model:                                              GLM   Df Residuals:                     1082\n",
       "Model Family:                                  Binomial   Df Model:                            6\n",
       "Link Function:                                    logit   Scale:                          1.0000\n",
       "Method:                                            IRLS   Log-Likelihood:                -743.18\n",
       "Date:                                  Sat, 02 Jan 2021   Deviance:                       1486.4\n",
       "Time:                                          21:41:47   Pearson chi2:                 1.09e+03\n",
       "No. Iterations:                                       4                                         \n",
       "Covariance Type:                              nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.2669      0.086     -3.106      0.002      -0.435      -0.098\n",
       "Lag1           0.0413      0.026      1.563      0.118      -0.010       0.093\n",
       "Lag2          -0.0584      0.027     -2.175      0.030      -0.111      -0.006\n",
       "Lag3           0.0161      0.027      0.602      0.547      -0.036       0.068\n",
       "Lag4           0.0278      0.026      1.050      0.294      -0.024       0.080\n",
       "Lag5           0.0145      0.026      0.549      0.583      -0.037       0.066\n",
       "Volume         0.0227      0.037      0.616      0.538      -0.050       0.095\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_fit = smf.glm('Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume',\n",
    "                data=weekly,\n",
    "                family=sm.families.Binomial()).fit()\n",
    "lg_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would seem that Lag2 is the only predictor statistically significant as its p-value is less than 0.05.\n",
    "\n",
    "(c) Compute the confusion matrix and overall farction of correct predictions. Explain what the confusion matrix is teling you about the types of mistakes made by logistic regression.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 54, 430],\n",
       "       [ 48, 557]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_pred = np.where(lg_fit.fittedvalues>0.5, 'Down', 'Up')\n",
    "real_direction = weekly['Direction']\n",
    "confusion_matrix(real_direction, lg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may conclude that the percentage of correct predictions on the training data is $(54+557)/1089$ wich is equal to $56.1065197\\%$. In other words $43.8934803\\%$ is the training error rate, which is often overly optimistic. We could also say that for weeks when the market goes up, the model is right $92.0661157\\%$ of the time $557/(48+557)$. For weeks when the market goes down, the model is right only $11.1570248\\%$ of the time $54/(54+430)$.\n",
    "\n",
    "(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with **Lag2** as the only predictor. Compute the confusion matrix and the overal fraction of correct predictions for the held out data (that is, the data from 2009 and 2010)\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = weekly['Year']<2009\n",
    "test_set = weekly[~train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>['Direction[Down]', 'Direction[Up]']</td> <th>  No. Observations:  </th>  <td>   985</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                            <td>GLM</td>                 <th>  Df Residuals:      </th>  <td>   983</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>                  <td>Binomial</td>               <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>                   <td>logit</td>                <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                          <td>IRLS</td>                 <th>  Log-Likelihood:    </th> <td> -675.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                      <td>Sat, 02 Jan 2021</td>           <th>  Deviance:          </th> <td>  1350.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                          <td>21:56:14</td>               <th>  Pearson chi2:      </th>  <td>  985.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>                    <td>4</td>                  <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>               <td>nonrobust</td>              <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -0.2033</td> <td>    0.064</td> <td>   -3.162</td> <td> 0.002</td> <td>   -0.329</td> <td>   -0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>   -0.0581</td> <td>    0.029</td> <td>   -2.024</td> <td> 0.043</td> <td>   -0.114</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Generalized Linear Model Regression Results                           \n",
       "================================================================================================\n",
       "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                  985\n",
       "Model:                                              GLM   Df Residuals:                      983\n",
       "Model Family:                                  Binomial   Df Model:                            1\n",
       "Link Function:                                    logit   Scale:                          1.0000\n",
       "Method:                                            IRLS   Log-Likelihood:                -675.27\n",
       "Date:                                  Sat, 02 Jan 2021   Deviance:                       1350.5\n",
       "Time:                                          21:56:14   Pearson chi2:                     985.\n",
       "No. Iterations:                                       4                                         \n",
       "Covariance Type:                              nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -0.2033      0.064     -3.162      0.002      -0.329      -0.077\n",
       "Lag2          -0.0581      0.029     -2.024      0.043      -0.114      -0.002\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_fit = smf.glm('Direction~Lag2',\n",
    "                data=weekly,\n",
    "                subset=train,\n",
    "                family=sm.families.Binomial()).fit()\n",
    "lg_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 34],\n",
       "       [ 5, 56]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_prob = lg_fit.predict(test_set)\n",
    "lg_pred = np.where(lg_prob>0.5,\n",
    "                  'Down', 'Up')\n",
    "real_direction = test_set['Direction']\n",
    "confusion_matrix(real_direction, lg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9+56)/len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we may conclude that the percentage of correct predictions on the test data is $(9+56)/104$ wich is equal to $62.5\\%$. In other words $37.5\\%$ is the test error rate. We could also say that for weeks when the market goes up, the model is right $91.8032787\\%$ of the time $56/(56+5)$. For weeks when the market goes down, the model is right only $20.9302326\\%$ of the time $9/(9+34)$.\n",
    "\n",
    "(e) Repeat (d) using LDA.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 34],\n",
       "       [ 5, 56]], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = weekly[train][['Lag2']]\n",
    "y_train = weekly[train]['Direction']\n",
    "\n",
    "X_test = test_set[['Lag2']]\n",
    "y_test = test_set['Direction']\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_fit = lda.fit(X_train, y_train)\n",
    "\n",
    "lda_pred = lda_fit.predict(X_test)\n",
    "confusion_matrix(y_test, lda_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get the same result as that obtained by the logistic regression.\n",
    "\n",
    "(f) Repeate (d) using QDA.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 43],\n",
       "       [ 0, 61]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda_fit = qda.fit(X_train, y_train)\n",
    "\n",
    "qda_pred = qda_fit.predict(X_test)\n",
    "confusion_matrix(y_test, qda_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we may conclude that the percentage of correct predictions on the test data is $58.6538462\\%$. In other words $41.3461538\\%$ is the test error rate. We could also say that for weeks when the market goes up, the model is right $100\\%$ of the time. For weeks when the market goes down, the model is right only $0\\%$ of the time. We may note, that QDA achieves a correctness of $58.6538462\\%$ even though the model chooses Up the whole time !\n",
    "\n",
    "(g) Repeat (d) using KNN with K = 1.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 22],\n",
       "       [31, 30]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn_pred = knn.fit(X_train, y_train)\\\n",
    "              .predict(X_test)\n",
    "confusion_matrix(y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we may conclude that the percentage of correct predictions on the test data is $49\\%$. In other words $51\\%$ is the test error rate. We could also say that for weeks when the market goes up, the model is right $49.18\\%$ of the time. For weeks when the market goes down, the model is right only $48.84\\%$ of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) Which of these methods appears to provide the best results on this data?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "Since the logistic regression and the LDA have the lowest test error rate, these two methods appear to provide the best results on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 36],\n",
       "       [ 8, 53]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression with Lag1:Lag2 terms\n",
    "lg_fit3 = smf.glm('Direction~Lag1*Lag2',\n",
    "                 data=weekly,\n",
    "                 subset=train,\n",
    "                 family=sm.families.Binomial()).fit()\n",
    "lg_fit3_prob = lg_fit3.predict(test_set)\n",
    "lg_fit3_pred = np.where(lg_fit3_prob>0.5,\n",
    "                       'Down', 'Up')\n",
    "confusion_matrix(y_test, lg_fit3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5769230769230769"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test == lg_fit3_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 38],\n",
       "       [ 6, 55]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA with Lag2 interaction with Lag1\n",
    "import patsy\n",
    "formula = 'Direction~Lag1*Lag2'\n",
    "y, X = patsy.dmatrices(formula, weekly, return_type='matrix')\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_fit3 = lda.fit(X, y[:,0])\n",
    "y_test, X_test = patsy.dmatrices(formula, test_set, return_type='matrix')\n",
    "lda_pred = lda_fit3.predict(X_test)\n",
    "lda_pred = np.where(lda_pred==1, 'Down', 'Up')\n",
    "confusion_matrix(test_set['Direction'], lda_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5769230769230769"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lda_pred == test_set['Direction']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
