{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Lab: Cross-Validation and the Bootstrap\n",
    "\n",
    "## 5.3.1 The Validation Set Approach\n",
    "\n",
    "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the **Auto** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 392 entries, 0 to 391\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   index         392 non-null    int64  \n",
      " 1   mpg           392 non-null    float64\n",
      " 2   cylinders     392 non-null    int64  \n",
      " 3   displacement  392 non-null    float64\n",
      " 4   horsepower    392 non-null    float64\n",
      " 5   weight        392 non-null    int64  \n",
      " 6   acceleration  392 non-null    float64\n",
      " 7   year          392 non-null    int64  \n",
      " 8   origin        392 non-null    int64  \n",
      " 9   name          392 non-null    object \n",
      "dtypes: float64(4), int64(5), object(1)\n",
      "memory usage: 30.8+ KB\n"
     ]
    }
   ],
   "source": [
    "auto = pd.read_csv('../data/Auto.csv',\n",
    "                  na_values='?')\\\n",
    "         .dropna()\\\n",
    "         .reset_index()\n",
    "auto.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by using the `DataFrame`'s `sample()` method to split the set of observations into two halves, by selecting a random subset of 196 observations out of the original 392 observations.\n",
    "\n",
    "To reproduce the result, we can specify the `random_state` option in the `sample()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auto = auto.sample(196, random_state=1)\n",
    "test_auto = auto[~auto.isin(train_auto)].dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `subset` option in `smf.ols()` to fit a linear regression using only the observations corresponding to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   316.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 08 Jan 2021</td> <th>  Prob (F-statistic):</th> <td>1.28e-42</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:33:15</td>     <th>  Log-Likelihood:    </th> <td> -592.07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   196</td>      <th>  AIC:               </th> <td>   1188.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   194</td>      <th>  BIC:               </th> <td>   1195.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>   40.3338</td> <td>    1.023</td> <td>   39.416</td> <td> 0.000</td> <td>   38.316</td> <td>   42.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th> <td>   -0.1596</td> <td>    0.009</td> <td>  -17.788</td> <td> 0.000</td> <td>   -0.177</td> <td>   -0.142</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 8.393</td> <th>  Durbin-Watson:     </th> <td>   1.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.015</td> <th>  Jarque-Bera (JB):  </th> <td>   8.787</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.516</td> <th>  Prob(JB):          </th> <td>  0.0124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.899</td> <th>  Cond. No.          </th> <td>    328.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.620\n",
       "Model:                            OLS   Adj. R-squared:                  0.618\n",
       "Method:                 Least Squares   F-statistic:                     316.4\n",
       "Date:                Fri, 08 Jan 2021   Prob (F-statistic):           1.28e-42\n",
       "Time:                        21:33:15   Log-Likelihood:                -592.07\n",
       "No. Observations:                 196   AIC:                             1188.\n",
       "Df Residuals:                     194   BIC:                             1195.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     40.3338      1.023     39.416      0.000      38.316      42.352\n",
       "horsepower    -0.1596      0.009    -17.788      0.000      -0.177      -0.142\n",
       "==============================================================================\n",
       "Omnibus:                        8.393   Durbin-Watson:                   1.808\n",
       "Prob(Omnibus):                  0.015   Jarque-Bera (JB):                8.787\n",
       "Skew:                           0.516   Prob(JB):                       0.0124\n",
       "Kurtosis:                       2.899   Cond. No.                         328.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit_auto = smf.ols('mpg~horsepower',\n",
    "                     data=auto,\n",
    "                     subset=train_auto.index).fit()\n",
    "lm_fit_auto.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the models's `predict()` method to estimate the response and calculate the MSE of the 196 observations in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.36190289258723"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_auto = lm_fit_auto.predict(test_auto)\n",
    "((pred_auto - test_auto['mpg'])**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the estimated test MSE for the linear regression fit is 23.36. We can use the `np.power()` function to estimate the test error for the quadratic and cubic regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.252690858350064"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit2_auto = smf.ols('mpg~horsepower+np.power(horsepower, 2)',\n",
    "                      data=auto,\n",
    "                      subset=train_auto.index).fit()\n",
    "pred2_auto = lm_fit2_auto.predict(test_auto)\n",
    "((pred2_auto - test_auto['mpg'])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.32560936589255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit3_auto = smf.ols('mpg~horsepower+np.power(horsepower, 2)+np.power(horsepower, 3)',\n",
    "                      data=auto,\n",
    "                      subset=train_auto.index).fit()\n",
    "pred3_auto = lm_fit3_auto.predict(test_auto)\n",
    "((pred3_auto - test_auto['mpg'])**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These error rates are 20.25 and 20.33 respectively. If we choose a diffrent training set instead, then we will obtain somewhat different errors on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auto = auto.sample(196, random_state=2)\n",
    "test_auto = auto[~auto.isin(train_auto)].dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.10853905288965"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit_auto = smf.ols('mpg~horsepower',\n",
    "                     data=auto,\n",
    "                     subset=train_auto.index).fit()\n",
    "pred_auto = lm_fit_auto.predict(test_auto)\n",
    "((pred_auto - test_auto['mpg'])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.722533470490276"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit2_auto = smf.ols('mpg~horsepower+np.power(horsepower, 2)',\n",
    "                      data=auto,\n",
    "                      subset=train_auto.index).fit()\n",
    "pred2_auto = lm_fit2_auto.predict(test_auto)\n",
    "((pred2_auto - test_auto['mpg'])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.921367860022265"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit3_auto = smf.ols('mpg~horsepower+np.power(horsepower, 2)+np.power(horsepower, 3)',\n",
    "                      data=auto,\n",
    "                      subset=train_auto.index).fit()\n",
    "pred3_auto = lm_fit3_auto.predict(test_auto)\n",
    "((pred3_auto - test_auto['mpg'])**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation set, we find that the validation set error rates for the models with linear, quadratic, and cubic terms are 25.11, 19.72, and 19.92, respectively.\n",
    "\n",
    "## 5.3.2 Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV estimate can be automatically computed for any generalized linear model using the `LeaveOneOut()` and `KFold()` functions from the sub-module `model_selection` in the `scikit-learn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, KFold, cross_val_score\n",
    "import  sklearn.linear_model as sk_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sk_lm.LinearRegression()\n",
    "\n",
    "X_train = train_auto['horsepower'].values.reshape(-1,1)\n",
    "y_train = train_auto['mpg']\n",
    "X_test = test_auto['horsepower'].values.reshape(-1,1)\n",
    "y_test = test_auto['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds: 392\n",
      "MSE: 24.231513517929226\n",
      "STD: 36.79731503640535\n"
     ]
    }
   ],
   "source": [
    "model = lm.fit(X_train, y_train)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "X = auto['horsepower'].values.reshape(-1,1)\n",
    "y = auto['mpg'].values.reshape(-1,1)\n",
    "\n",
    "scores = cross_val_score(lm, X, y,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         cv=loo,\n",
    "                         n_jobs=1)\n",
    "print(f'Folds: {len(scores)}'\n",
    "      f'\\nMSE: {np.mean(np.abs(scores))}' # since the scoring function returns a negative MSE.\n",
    "      f'\\nSTD: {np.std(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our cross-validation estimate for the test error is approximately 24.23.\n",
    "\n",
    "We can repeat this procedure for increasingly complex polynomial fits. In this case, we'll do it in a *for loop* to iteratively fit polynomial regressions for polynomials of order `i=1` to `i=5`, and compute the associated cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree-1 polynomial\n",
      "MSE: 24.23151351792922\n",
      "STD: 36.797315036405344\n",
      "=================================\n",
      "Degree-2 polynomial\n",
      "MSE: 19.248213124489748\n",
      "STD: 34.99844615178233\n",
      "=================================\n",
      "Degree-3 polynomial\n",
      "MSE: 19.33498406406717\n",
      "STD: 35.76513567797019\n",
      "=================================\n",
      "Degree-4 polynomial\n",
      "MSE: 19.424430308775744\n",
      "STD: 35.683352759230665\n",
      "=================================\n",
      "Degree-5 polynomial\n",
      "MSE: 19.0332148474514\n",
      "STD: 35.317311409482386\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "    X_current = poly.fit_transform(X)\n",
    "    model = lm.fit(X_current, y)\n",
    "    scores = cross_val_score(lm, X_current, y,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv=loo,\n",
    "                            n_jobs=1)\n",
    "    print(f'Degree-{i} polynomial\\n'\n",
    "          f'MSE: {np.mean(np.abs(scores))}\\n'\n",
    "          f'STD: {np.std(scores)}\\n'\n",
    "          '=================================')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a sharp drop in the estimated test MSE between the linear and quafratic fits, but then no clear improvement from using higher-order polynomials.\n",
    "\n",
    "## 5.3.3 k-Fold Cross-Validation\n",
    "The `KFold()` function can be used to implement *k*-fold CV. Below we use `k=10`, a common choice for *k*, on the **Auto** data set. We will also obtain the polynomial fits of orders one to ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree-1 polynomial\n",
      "MSE: 24.097675731883058\n",
      "STD: 4.818054666704995\n",
      "====================================\n",
      "Degree-2 polynomial\n",
      "MSE: 19.178889864889662\n",
      "STD: 5.126393446517313\n",
      "====================================\n",
      "Degree-3 polynomial\n",
      "MSE: 19.213859523704357\n",
      "STD: 5.143687485487353\n",
      "====================================\n",
      "Degree-4 polynomial\n",
      "MSE: 19.21280701911446\n",
      "STD: 4.926661024804993\n",
      "====================================\n",
      "Degree-5 polynomial\n",
      "MSE: 18.75797963141857\n",
      "STD: 4.70323502370233\n",
      "====================================\n",
      "Degree-6 polynomial\n",
      "MSE: 18.63518869983694\n",
      "STD: 4.509539871936917\n",
      "====================================\n",
      "Degree-7 polynomial\n",
      "MSE: 18.82096682619666\n",
      "STD: 4.564907863618866\n",
      "====================================\n",
      "Degree-8 polynomial\n",
      "MSE: 18.97573372824936\n",
      "STD: 4.7117057342608595\n",
      "====================================\n",
      "Degree-9 polynomial\n",
      "MSE: 18.93746190765147\n",
      "STD: 4.869598846436348\n",
      "====================================\n",
      "Degree-10 polynomial\n",
      "MSE: 18.79001024796583\n",
      "STD: 4.840922846529729\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "    X_current = poly.fit_transform(X)\n",
    "    scores = cross_val_score(lm, X_current, y,\n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             cv=crossvalidation,\n",
    "                             n_jobs=1)\n",
    "    print(f'Degree-{i} polynomial\\n'\n",
    "          f'MSE: {np.mean(np.abs(scores))}\\n'\n",
    "          f'STD: {np.std(scores)}\\n'\n",
    "          '====================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.\n",
    "\n",
    "## 5.3.4 The Bootstrap\n",
    "We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the **Auto** data set.\n",
    "\n",
    "### Estimating the Accuracy of a Statistic of Interest\n",
    "One of the greate advantages of the bootstrap approach is that it can be applied in almost all situations. \n",
    "\n",
    "In this setting, we use the **Portfolio** data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.895251</td>\n",
       "      <td>-0.234924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.562454</td>\n",
       "      <td>-0.885176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.417090</td>\n",
       "      <td>0.271888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.044356</td>\n",
       "      <td>-0.734198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.315568</td>\n",
       "      <td>0.841983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X         Y\n",
       "1 -0.895251 -0.234924\n",
       "2 -1.562454 -0.885176\n",
       "3 -0.417090  0.271888\n",
       "4  1.044356 -0.734198\n",
       "5 -0.315568  0.841983"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio = pd.read_csv('../data/Portfolio.csv',\n",
    "                        na_values='?',\n",
    "                        index_col=0)\n",
    "portfolio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we first define a `alpha()` function, which takes as input the `(X, Y)` data to estimate $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(X, Y):\n",
    "    result = (np.var(Y) - np.cov(X, Y))/(np.var(X) + np.var(Y) - 2*np.cov(X, Y))\n",
    "    return result[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5766511516104116"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = portfolio.X[0:100]\n",
    "Y = portfolio.Y[0:100]\n",
    "alpha(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the estimated $\\alpha$ using all 100 observations is 0.57665115.\n",
    "\n",
    "The next command uses the `sample()` method to randomly select 100 observations from the range 1 to 100, with replacement. This is equivalent to constructing a new bootstrap data set and recomputing $\\hat{\\alpha}$based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04481519, 0.61098518],\n",
       "       [0.61098518, 0.04646865]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_sample = portfolio.sample(frac=1, replace=True)\n",
    "X_sample = portfolio_sample.X[0:100]\n",
    "Y_sample = portfolio_sample.Y[0:100]\n",
    "alpha(X_sample, Y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for $\\alpha$, and computing the resulting standard deviation. Below we produce 1,000 bootstrap estimates for $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot(df, fun, n, *, x='X', y='Y'):\n",
    "    tresult = []\n",
    "    for i in range(0, n):\n",
    "        dfsample = df.sample(frac=1, replace=True)\n",
    "        X = dfsample[x]\n",
    "        Y = dfsample[y]\n",
    "        result = fun(X, Y)\n",
    "        tresult.append(result)\n",
    "    estimate = sum(tresult)/n\n",
    "    std_est = np.std(tresult, axis=0)\n",
    "    print('Bootstrap Statistics:\\n'\n",
    "         f'Estimate: {estimate}\\n'\n",
    "         f'STD: {std_est}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Statistics:\n",
      "Estimate: 0.5825695998321582\n",
      "STD: 0.09024505084391195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boot(portfolio, alpha, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result shows that using the original data, $\\hat{\\alpha} = 0.582$, and that the bootstrap estimate for $SE(\\hat{\\alpha})$ is 0.09.\n",
    "\n",
    "### Estimating the Accuracy of a Linear Regression Model\n",
    "The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for $\\beta_0$ and $\\beta_1$, the intercept and slope terms for the linear regression model that uses **horsepower** to predict **mpg** in the **Auto** data set.\n",
    "\n",
    "We first define a simple function, `coef()`, which takes in the `(X, Y)` parameters and returns the intercept and slope estimates for the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef(X, Y):\n",
    "    lm = sk_lm.LinearRegression()\n",
    "    model = lm.fit(X.values.reshape(-1,1), Y)\n",
    "    return np.append(model.intercept_, model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.93586102, -0.15784473])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef(auto.horsepower, \n",
    "     auto.mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.20546725, -0.16629016])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_sample = auto.sample(frac=1, replace=True)\n",
    "coef(auto_sample.horsepower,\n",
    "     auto_sample.mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.09773013, -0.1672969 ])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_sample = auto.sample(frac=1, replace=True)\n",
    "coef(auto_sample.horsepower.values.reshape(-1,1),\n",
    "     auto_sample.mpg.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `boot()` function to compute the standard error of 1,000 bootstrap estimates for the intercept and slope terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Statistics:\n",
      "Estimate: [39.971905   -0.15811471]\n",
      "STD: [0.8502544  0.00739021]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boot(auto, coef, 1000, x='horsepower', y='mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the bootstrap estimates for $SE(\\hat{\\beta}_0$) is 0.85, and that the bootstrap estimate for $SE(\\hat{\\beta}_1)$ is 0.0074.\n",
    "\n",
    "The standard errors for the regression coefficients in a linear model can be computed using the standard formulas. These can be obtained using the `statsmodels` library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   599.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 08 Jan 2021</td> <th>  Prob (F-statistic):</th> <td>7.03e-81</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:39:33</td>     <th>  Log-Likelihood:    </th> <td> -1178.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2361.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   390</td>      <th>  BIC:               </th> <td>   2369.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>   39.9359</td> <td>    0.717</td> <td>   55.660</td> <td> 0.000</td> <td>   38.525</td> <td>   41.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th> <td>   -0.1578</td> <td>    0.006</td> <td>  -24.489</td> <td> 0.000</td> <td>   -0.171</td> <td>   -0.145</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>16.432</td> <th>  Durbin-Watson:     </th> <td>   0.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  17.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.492</td> <th>  Prob(JB):          </th> <td>0.000175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.299</td> <th>  Cond. No.          </th> <td>    322.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.606\n",
       "Model:                            OLS   Adj. R-squared:                  0.605\n",
       "Method:                 Least Squares   F-statistic:                     599.7\n",
       "Date:                Fri, 08 Jan 2021   Prob (F-statistic):           7.03e-81\n",
       "Time:                        23:39:33   Log-Likelihood:                -1178.7\n",
       "No. Observations:                 392   AIC:                             2361.\n",
       "Df Residuals:                     390   BIC:                             2369.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     39.9359      0.717     55.660      0.000      38.525      41.347\n",
       "horsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n",
       "==============================================================================\n",
       "Omnibus:                       16.432   Durbin-Watson:                   0.920\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\n",
       "Skew:                           0.492   Prob(JB):                     0.000175\n",
       "Kurtosis:                       3.299   Cond. No.                         322.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit = smf.ols('mpg~horsepower',\n",
    "                data=auto).fit()\n",
    "lm_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data, there is expected to be a better correspondence between the bootstrap estimates and the standard estimates of $SE(\\hat{\\beta}_0)$, $SE(\\hat{\\beta}_1)$ and $SE(\\hat{\\beta}_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef2(X, Y):\n",
    "    lm = sk_lm.LinearRegression()\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_current = poly.fit_transform(X.values.reshape(-1,1))\n",
    "    model = lm.fit(X_current, Y)\n",
    "    return np.append(model.intercept_, model.coef_[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Statistics:\n",
      "Estimate: [ 5.69371890e+01 -4.66587462e-01  1.23128996e-03]\n",
      "STD: [2.09911479e+00 3.32260259e-02 1.19470045e-04]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boot(auto, coef2, 1000, x='horsepower', y='mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   428.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 08 Jan 2021</td> <th>  Prob (F-statistic):</th> <td>5.40e-99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:54:59</td>     <th>  Log-Likelihood:    </th> <td> -1133.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2272.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   389</td>      <th>  BIC:               </th> <td>   2284.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>               <td>   56.9001</td> <td>    1.800</td> <td>   31.604</td> <td> 0.000</td> <td>   53.360</td> <td>   60.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th>              <td>   -0.4662</td> <td>    0.031</td> <td>  -14.978</td> <td> 0.000</td> <td>   -0.527</td> <td>   -0.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.power(horsepower, 2)</th> <td>    0.0012</td> <td>    0.000</td> <td>   10.080</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>16.158</td> <th>  Durbin-Watson:     </th> <td>   1.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  30.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.218</td> <th>  Prob(JB):          </th> <td>2.20e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.299</td> <th>  Cond. No.          </th> <td>1.29e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.29e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.688\n",
       "Model:                            OLS   Adj. R-squared:                  0.686\n",
       "Method:                 Least Squares   F-statistic:                     428.0\n",
       "Date:                Fri, 08 Jan 2021   Prob (F-statistic):           5.40e-99\n",
       "Time:                        23:54:59   Log-Likelihood:                -1133.2\n",
       "No. Observations:                 392   AIC:                             2272.\n",
       "Df Residuals:                     389   BIC:                             2284.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===========================================================================================\n",
       "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------\n",
       "Intercept                  56.9001      1.800     31.604      0.000      53.360      60.440\n",
       "horsepower                 -0.4662      0.031    -14.978      0.000      -0.527      -0.405\n",
       "np.power(horsepower, 2)     0.0012      0.000     10.080      0.000       0.001       0.001\n",
       "==============================================================================\n",
       "Omnibus:                       16.158   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               30.662\n",
       "Skew:                           0.218   Prob(JB):                     2.20e-07\n",
       "Kurtosis:                       4.299   Cond. No.                     1.29e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.29e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_fit = smf.ols('mpg~horsepower+np.power(horsepower,2)',\n",
    "                data=auto).fit()\n",
    "lm_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
